[32m2025-12-03 01:26:04[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate[0m:[36m311[0m - [1mVerbosity set to DEBUG[0m
[32m2025-12-03 01:26:04[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m458[0m - [34m[1m`group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lmms-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.[0m
[32m2025-12-03 01:26:05[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m484[0m - [34m[1mFile _default_template.yaml in /home/train01/miraj/lmms_eval/lmms_eval/tasks/video-tt could not be loaded as a task or group[0m
[32m2025-12-03 01:26:05[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.tasks[0m:[36m_get_task_and_group[0m:[36m484[0m - [34m[1mFile illusionvqa.yaml in /home/train01/miraj/lmms_eval/lmms_eval/tasks/illusionvqa could not be loaded as a task or group[0m
[32m2025-12-03 01:26:05[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m400[0m - [1mEvaluation tracker args: {'output_path': './results/full_logs/selected_longvideobench_clip_frames_k64_aks_results'}[0m
[32m2025-12-03 01:26:05[0m | [1mINFO    [0m | [36m__main__[0m:[36mcli_evaluate_single[0m:[36m480[0m - [1mSelected Tasks: ['longvideobench_custom'][0m
[32m2025-12-03 01:26:05[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36msimple_evaluate[0m:[36m161[0m - [1mSetting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234[0m
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
OpenCLIP not installed
force sample: False
Loaded LLaVA model: ../LLaVA-NeXT-Video-7B-Qwen2
Loading vision tower: google/siglip-so400m-patch14-384
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.22s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.00it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.09s/it]
Model Class: LlavaQwenForCausalLM
[32m2025-12-03 01:26:13[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36m__init__[0m:[36m219[0m - [1mUsing 1 devices with tensor parallelism[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m402[0m - [1mRunning on rank 0 (local rank 0)[0m
[32m2025-12-03 01:26:14[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.caching.cache[0m:[36mload_from_cache[0m:[36m34[0m - [34m[1mrequests-longvideobench_custom-0shot-rank0-world_size1-tokenizer is not cached, generating...[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mlmms_eval.api.task[0m:[36mbuild_all_requests[0m:[36m427[0m - [1mBuilding contexts for longvideobench_custom on rank 0...[0m
  0%|          | 0/1337 [00:00<?, ?it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1129/1337 [00:00<00:00, 11288.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1337/1337 [00:00<00:00, 11208.46it/s]
[32m2025-12-03 01:26:14[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m460[0m - [34m[1mTask: longvideobench_custom; number of requests on this rank: 1337[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mlmms_eval.evaluator[0m:[36mevaluate[0m:[36m495[0m - [1mRunning generate_until requests[0m
Model Responding:   0%|          | 0/1337 [00:00<?, ?it/s][32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m408[0m - [1mDEBUG: base_cache_dir = /home/train01/.cache/huggingface/[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m417[0m - [1mDEBUG: cache_name from YAML = longvideobench_custom_cache[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m425[0m - [1mDEBUG: final cache_dir = /home/train01/.cache/huggingface/longvideobench_custom_cache[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m429[0m - [1mDEBUG: video_filename from doc = 86CxyhFV9MI.mp4[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m445[0m - [1mDEBUG: constructed video_path = /home/train01/.cache/huggingface/longvideobench_custom_cache/videos/86CxyhFV9MI.mp4[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m446[0m - [1mDEBUG: video_path exists? True[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m484[0m - [1m[longvideobench_custom] Video: 86CxyhFV9MI.mp4 | Selected 64 frames[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mgenerate_until[0m:[36m626[0m - [1m[CUSTOM_FRAMES] Detected custom frame selection for doc_id=0[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mgenerate_until[0m:[36m629[0m - [1m[CUSTOM_FRAMES] Will load 64 specific frames[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mload_video[0m:[36m375[0m - [1m[CUSTOM_FRAMES] Received 64 custom frames[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mload_video[0m:[36m379[0m - [1m[CUSTOM_FRAMES] Subsampling from 64 to 48 frames[0m
[32m2025-12-03 01:26:14[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mload_video[0m:[36m386[0m - [1m[CUSTOM_FRAMES] Using 48 custom frames instead of uniform sampling[0m
[32m2025-12-03 01:26:24[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mgenerate_until[0m:[36m742[0m - [34m[1mQuestion: <image>
In the video, which subtitles appear at the same time as the man with black hair, dressed in grey clothes with black sleeves, on stage?
A. promisc has come to an end, in and run away countless times, i was just scared, i still
B. run away countless times, i was just scared, i still and front of our crown, like a world of souls,
C. promisc has come to an end, in and front of our crown, like a world of souls,
D. promisc has come to an end, in and captain of the godson, three three three three three three
Answer with the option's letter from the given choices directly.[0m
[32m2025-12-03 01:26:24[0m | [34m[1mDEBUG   [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mgenerate_until[0m:[36m743[0m - [34m[1mAnswer: B.[0m
Model Responding:   0%|          | 1/1337 [00:09<3:41:57,  9.97s/it][32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m408[0m - [1mDEBUG: base_cache_dir = /home/train01/.cache/huggingface/[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m417[0m - [1mDEBUG: cache_name from YAML = longvideobench_custom_cache[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m425[0m - [1mDEBUG: final cache_dir = /home/train01/.cache/huggingface/longvideobench_custom_cache[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m429[0m - [1mDEBUG: video_filename from doc = 86CxyhFV9MI.mp4[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m445[0m - [1mDEBUG: constructed video_path = /home/train01/.cache/huggingface/longvideobench_custom_cache/videos/86CxyhFV9MI.mp4[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m446[0m - [1mDEBUG: video_path exists? True[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mutils[0m:[36mlongvideobench_custom_doc_to_visual[0m:[36m484[0m - [1m[longvideobench_custom] Video: 86CxyhFV9MI.mp4 | Selected 64 frames[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mgenerate_until[0m:[36m626[0m - [1m[CUSTOM_FRAMES] Detected custom frame selection for doc_id=1[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mgenerate_until[0m:[36m629[0m - [1m[CUSTOM_FRAMES] Will load 64 specific frames[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mload_video[0m:[36m375[0m - [1m[CUSTOM_FRAMES] Received 64 custom frames[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mload_video[0m:[36m379[0m - [1m[CUSTOM_FRAMES] Subsampling from 64 to 48 frames[0m
[32m2025-12-03 01:26:24[0m | [1mINFO    [0m | [36mlmms_eval.models.simple.llava_vid[0m:[36mload_video[0m:[36m386[0m - [1m[CUSTOM_FRAMES] Using 48 custom frames instead of uniform sampling[0m
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/train01/miraj/lmms_eval/lmms_eval/__main__.py", line 549, in <module>
    cli_evaluate()
    ~~~~~~~~~~~~^^
  File "/home/train01/miraj/lmms_eval/lmms_eval/__main__.py", line 368, in cli_evaluate
    raise e
  File "/home/train01/miraj/lmms_eval/lmms_eval/__main__.py", line 349, in cli_evaluate
    results, samples = cli_evaluate_single(args)
                       ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/home/train01/miraj/lmms_eval/lmms_eval/__main__.py", line 484, in cli_evaluate_single
    results = evaluator.simple_evaluate(
        model=args.model,
    ...<28 lines>...
        **request_caching_args,
    )
  File "/home/train01/miraj/lmms_eval/lmms_eval/utils.py", line 536, in _wrapper
    return fn(*args, **kwargs)
  File "/home/train01/miraj/lmms_eval/lmms_eval/evaluator.py", line 268, in simple_evaluate
    results = evaluate(
        lm=lm,
    ...<13 lines>...
        eval_server_launcher=eval_launcher,
    )
  File "/home/train01/miraj/lmms_eval/lmms_eval/utils.py", line 536, in _wrapper
    return fn(*args, **kwargs)
  File "/home/train01/miraj/lmms_eval/lmms_eval/evaluator.py", line 506, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)  # Choiszt run generate until
  File "/home/train01/miraj/lmms_eval/lmms_eval/models/simple/llava_vid.py", line 727, in generate_until
    output_ids = self.model.generate(
        inputs=input_ids,
    ...<9 lines>...
        max_new_tokens=gen_kwargs["max_new_tokens"],
    )
  File "/home/train01/miraj/lmms_eval/venv/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/train01/miraj/LLaVA-NeXT/llava/model/language_model/llava_qwen.py", line 135, in generate
    return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/train01/miraj/lmms_eval/venv/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/train01/miraj/lmms_eval/venv/lib/python3.13/site-packages/transformers/generation/utils.py", line 1544, in generate
    return self.greedy_search(
           ~~~~~~~~~~~~~~~~~~^
        input_ids,
        ^^^^^^^^^^
    ...<9 lines>...
        **model_kwargs,
        ^^^^^^^^^^^^^^^
    )
    ^
  File "/home/train01/miraj/lmms_eval/venv/lib/python3.13/site-packages/transformers/generation/utils.py", line 2404, in greedy_search
    outputs = self(
        **model_inputs,
    ...<2 lines>...
        output_hidden_states=output_hidden_states,
    )
  File "/home/train01/miraj/lmms_eval/venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/train01/miraj/lmms_eval/venv/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/train01/miraj/lmms_eval/venv/lib/python3.13/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/train01/miraj/LLaVA-NeXT/llava/model/language_model/llava_qwen.py", line 103, in forward
    return super().forward(
           ~~~~~~~~~~~~~~~^
        input_ids=input_ids,
        ^^^^^^^^^^^^^^^^^^^^
    ...<8 lines>...
        return_dict=return_dict,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/train01/miraj/lmms_eval/venv/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1187, in forward
    logits = logits.float()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.77 GiB. GPU 1 has a total capacity of 23.56 GiB of which 4.50 GiB is free. Including non-PyTorch memory, this process has 19.05 GiB memory in use. Of the allocated memory 12.83 GiB is allocated by PyTorch, and 5.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Model Responding:   0%|          | 1/1337 [00:18<6:43:56, 18.14s/it]
